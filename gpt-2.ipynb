{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nimport os\n\ndef setup_environment(base_dir: str = '/kaggle/working/GPT-2-Project'):\n    data_dir  = os.path.join(base_dir, 'data')\n    model_dir = os.path.join(base_dir, 'models')\n    os.makedirs(data_dir, exist_ok=True)\n    os.makedirs(model_dir, exist_ok=True)\n    return {\n        'train_txt':      os.path.join(data_dir, 'TinyStories-train.txt'),\n        'val_txt':        os.path.join(data_dir, 'TinyStories-valid.txt'),\n        'model_ckpt':     os.path.join(model_dir, 'best_gpt2.pt'),\n        'full_ckpt':      os.path.join(model_dir, 'checkpoint_full.pt'),\n        'tokenizer_json': os.path.join(model_dir, 'tokenizer.json'),\n    }\n\ndef download_dataset(paths):\n    \n    dataset = load_dataset(\"roneneldan/TinyStories\")\n    \n    \n    with open(paths['train_txt'], 'w', encoding='utf-8') as f:\n        for story in dataset['train']['text']:\n            f.write(story + '\\n')\n    \n    \n    with open(paths['val_txt'], 'w', encoding='utf-8') as f:\n        for story in dataset['validation']['text']:\n            f.write(story + '\\n')\n\n\npaths = setup_environment()\ndownload_dataset(paths)\n\nprint(\"DONE:\")\nprint(f\"train: {paths['train_txt']}\")\nprint(f\"val: {paths['val_txt']}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.idle":"2025-05-26T23:04:14.197510Z","shell.execute_reply.started":"2025-05-26T23:03:59.315124Z","shell.execute_reply":"2025-05-26T23:04:14.196753Z"}},"outputs":[{"name":"stdout","text":"DONE:\ntrain: /kaggle/working/GPT-2-Project/data/TinyStories-train.txt\nval: /kaggle/working/GPT-2-Project/data/TinyStories-valid.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:04:14.199531Z","iopub.execute_input":"2025-05-26T23:04:14.200164Z","iopub.status.idle":"2025-05-26T23:04:15.903063Z","shell.execute_reply.started":"2025-05-26T23:04:14.200137Z","shell.execute_reply":"2025-05-26T23:04:15.902445Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# 2. Model Implementation\n# =============================================================================\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        self.qkv = nn.Linear(d_model, 3 * d_model)\n        self.proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n        B, T, C = x.shape\n        q, k, v = self.qkv(x).split(self.d_model, dim=2)\n\n        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n\n        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n        if mask is not None:\n            attn = attn.masked_fill(mask == 0, float('-inf'))\n        attn = F.softmax(attn, dim=-1)\n        attn = self.dropout(attn)\n\n        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n        return self.proj(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:04:15.903728Z","iopub.execute_input":"2025-05-26T23:04:15.903963Z","iopub.status.idle":"2025-05-26T23:04:15.911983Z","shell.execute_reply.started":"2025-05-26T23:04:15.903945Z","shell.execute_reply":"2025-05-26T23:04:15.910252Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class PositionWiseFFN(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.net(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:04:15.912791Z","iopub.execute_input":"2025-05-26T23:04:15.913310Z","iopub.status.idle":"2025-05-26T23:04:23.135410Z","shell.execute_reply.started":"2025-05-26T23:04:15.913284Z","shell.execute_reply":"2025-05-26T23:04:23.134671Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.ffn = PositionWiseFFN(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n        attn_out = self.self_attn(self.norm1(x), mask)\n        x = x + self.dropout(attn_out)\n        ffn_out = self.ffn(self.norm2(x))\n        x = x + self.dropout(ffn_out)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:04:23.136235Z","iopub.execute_input":"2025-05-26T23:04:23.136499Z","iopub.status.idle":"2025-05-26T23:04:23.148406Z","shell.execute_reply.started":"2025-05-26T23:04:23.136480Z","shell.execute_reply":"2025-05-26T23:04:23.147652Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class GPT2(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int = 768, num_layers: int = 12,\n                 num_heads: int = 12, d_ff: int = 3072, max_seq: int = 1024, dropout: float = 0.1):\n        super().__init__()\n        self.token_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Parameter(torch.zeros(1, max_seq, d_model))\n        self.dropout = nn.Dropout(dropout)\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_layers)\n        ])\n        self.norm = nn.LayerNorm(d_model)\n        self.out = nn.Linear(d_model, vocab_size)\n        self.max_seq_len = max_seq\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.LayerNorm):\n            torch.nn.init.zeros_(module.bias)\n            torch.nn.init.ones_(module.weight)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n        B, T = x.shape\n        token_emb = self.token_emb(x) * math.sqrt(self.token_emb.embedding_dim)\n        pos_emb = self.pos_emb[:, :T, :]\n        x = self.dropout(token_emb + pos_emb)\n\n        for layer in self.layers:\n            x = layer(x, mask)\n        x = self.norm(x)\n        return self.out(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:04:23.149314Z","iopub.execute_input":"2025-05-26T23:04:23.149671Z","iopub.status.idle":"2025-05-26T23:04:23.164903Z","shell.execute_reply.started":"2025-05-26T23:04:23.149638Z","shell.execute_reply":"2025-05-26T23:04:23.164204Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# =============================================================================\n# 3. Dataset & Tokenizer\n# =============================================================================\nclass TinyStoriesDataset(Dataset):\n    def __init__(self, file_path: str, tokenizer: Tokenizer, max_len: int = 1024):\n        with open(file_path, 'r', encoding='utf-8') as f:\n            self.lines = [l.strip() for l in f if l.strip()]\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.pad_id = tokenizer.token_to_id(\"[PAD]\") or 0\n\n    def __len__(self) -> int:\n        return len(self.lines)\n\n    def __getitem__(self, idx: int):\n        tokens = self.tokenizer.encode(self.lines[idx]).ids[:self.max_len]\n        tokens = tokens + [self.pad_id] * (self.max_len - len(tokens))\n        return torch.tensor(tokens[:-1], dtype=torch.long), torch.tensor(tokens[1:], dtype=torch.long)\n\ndef train_tokenizer(train_file: str, save_path: str, vocab_size: int = 10000) -> Tokenizer:\n    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n    tokenizer.pre_tokenizer = Whitespace()\n    trainer = BpeTrainer(\n        special_tokens=[\"[UNK]\", \"[PAD]\", \"[BOS]\", \"[EOS]\"],\n        vocab_size=vocab_size\n    )\n    tokenizer.train([train_file], trainer)\n    tokenizer.save(save_path)\n    return tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:04:23.165572Z","iopub.execute_input":"2025-05-26T23:04:23.165823Z","iopub.status.idle":"2025-05-26T23:04:23.177900Z","shell.execute_reply.started":"2025-05-26T23:04:23.165795Z","shell.execute_reply":"2025-05-26T23:04:23.177240Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# =============================================================================\n# 4. Training & Evaluation\n# =============================================================================\ndef train_model(config, paths):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    if not os.path.exists(paths['tokenizer_json']):\n        print(\"Training tokenizer...\")\n        tokenizer = train_tokenizer(paths['train_txt'], paths['tokenizer_json'], config['vocab_size'])\n    else:\n        tokenizer = Tokenizer.from_file(paths['tokenizer_json'])\n\n    full_train_ds = TinyStoriesDataset(paths['train_txt'], tokenizer, config['max_seq_len'])\n    full_val_ds = TinyStoriesDataset(paths['val_txt'], tokenizer, config['max_seq_len'])\n\n    train_indices = random.sample(range(len(full_train_ds)), int(0.1 * len(full_train_ds)))\n    val_indices = random.sample(range(len(full_val_ds)), int(0.1 * len(full_val_ds)))\n\n    train_ds = Subset(full_train_ds, train_indices)\n    val_ds = Subset(full_val_ds, val_indices)\n\n    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=config['batch_size'])\n\n    model = GPT2(\n        vocab_size=config['vocab_size'],\n        d_model=config['d_model'],\n        num_layers=config['num_layers'],\n        num_heads=config['num_heads'],\n        d_ff=config['d_ff'],\n        max_seq=config['max_seq_len']\n    ).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n    criterion = nn.CrossEntropyLoss(ignore_index=full_train_ds.pad_id)\n    best_val_loss = float('inf')\n\n    for epoch in range(1, config['epochs'] + 1):\n        model.train()\n        train_loss = 0\n        mask = torch.tril(torch.ones(config['max_seq_len']-1, config['max_seq_len']-1, device=device)).bool()\n\n        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs, mask)\n            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            train_loss += loss.item()\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs, mask)\n                val_loss += criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1)).item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader)\n        print(f\"Epoch {epoch:02d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(model.state_dict(), paths['model_ckpt'])\n            print(f\"Saved best model (val_loss={best_val_loss:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:04:23.180006Z","iopub.execute_input":"2025-05-26T23:04:23.180356Z","iopub.status.idle":"2025-05-26T23:04:23.194477Z","shell.execute_reply.started":"2025-05-26T23:04:23.180337Z","shell.execute_reply":"2025-05-26T23:04:23.193940Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# 5. Generation & Evaluation\n# ============================================================================\ndef generate_text(model, tokenizer, prompt=\"\", max_length=50, temperature=1.0, top_k=None, device='cuda'):\n    model.eval()\n    tokens = tokenizer.encode(prompt).ids\n    for _ in range(max_length):\n        input_ids = torch.tensor([tokens[-model.max_seq_len:]], device=device)\n        mask = torch.tril(torch.ones(input_ids.size(1), input_ids.size(1), device=device)).bool()\n        with torch.no_grad():\n            logits = model(input_ids, mask)[0, -1, :]\n        if temperature != 1.0:\n            logits = logits / temperature\n        if top_k is not None:\n            v, _ = torch.topk(logits, top_k)\n            logits[logits < v[-1]] = -float('Inf')\n        probs = F.softmax(logits, dim=-1)\n        next_token = torch.multinomial(probs, num_samples=1).item()\n        tokens.append(next_token)\n        if next_token == tokenizer.token_to_id(\"[PAD]\"):\n            break\n    return tokenizer.decode(tokens)\n\ndef calculate_perplexity(model, data_loader, device='cuda'):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    mask = torch.tril(torch.ones(model.max_seq_len-1, model.max_seq_len-1, device=device)).bool()\n    with torch.no_grad():\n        for inputs, targets in tqdm(data_loader, desc=\"Calculating Perplexity\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs, mask)\n            loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)),\n                                 targets.view(-1),\n                                 ignore_index=0,\n                                 reduction='sum')\n            total_loss += loss.item()\n            total_tokens += (targets != 0).sum().item()\n    avg_loss = total_loss / total_tokens\n    return math.exp(avg_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:04:23.195196Z","iopub.execute_input":"2025-05-26T23:04:23.195443Z","iopub.status.idle":"2025-05-26T23:04:23.210091Z","shell.execute_reply.started":"2025-05-26T23:04:23.195422Z","shell.execute_reply":"2025-05-26T23:04:23.209530Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# =============================================================================\n# 6. Main Execution\n# =============================================================================\nif __name__ == '__main__':\n    config ={\n    'vocab_size': 10000,\n    'd_model': 512,\n    'num_layers': 6,\n    'num_heads': 8,\n    'd_ff': 2048,\n    'max_seq_len': 256,\n    'batch_size': 16,\n    'epochs': 5,\n    'lr': 3e-4,\n    }\n\n\n    train_model(config, paths)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = GPT2(\n        vocab_size=config['vocab_size'],\n        d_model=config['d_model'],\n        num_layers=config['num_layers'],\n        num_heads=config['num_heads'],\n        d_ff=config['d_ff'],\n        max_seq=config['max_seq_len']\n    ).to(device)\n    model.load_state_dict(torch.load(paths['model_ckpt']))\n\n    tokenizer = Tokenizer.from_file(paths['tokenizer_json'])\n\n    print(\"\\nGenerated Samples:\")\n    for prompt in [\"Once upon a time\", \"In a magical kingdom\", \"The scientist discovered\"]:\n        generated = generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7)\n        print(f\"\\nPrompt: {prompt}\\nGenerated: {generated}\\n\")\n\n    full_val_ds = TinyStoriesDataset(paths['val_txt'], tokenizer, config['max_seq_len'])\n    val_indices = random.sample(range(len(full_val_ds)), int(0.1 * len(full_val_ds)))\n    val_ds = Subset(full_val_ds, val_indices)\n    val_loader = DataLoader(val_ds, batch_size=config['batch_size'])\n    perplexity = calculate_perplexity(model, val_loader)\n    print(f\"\\nModel Perplexity: {perplexity:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:04:23.211052Z","iopub.execute_input":"2025-05-26T23:04:23.211311Z","execution_failed":"2025-05-27T02:55:29.373Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/71332 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8d2836854df463797c5c146e53d61c7"}},"metadata":{}},{"name":"stdout","text":"Epoch 01 | Train Loss: 2.4178 | Val Loss: 2.1148\nSaved best model (val_loss=2.1148)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/71332 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4861302acb334dbb8550d0d8a9423a8d"}},"metadata":{}}],"execution_count":null}]}